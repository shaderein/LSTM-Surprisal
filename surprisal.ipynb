{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "from utils.data import preprocess, word2idx, Dictionary\n",
    "from nltk import sent_tokenize"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "print(torch.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load pretrained models\n",
    "`/LSTM_40m` contains a batch of pretrained models. The name convention is:\n",
    "\n",
    "`LSTM_[Hidden Units]_[Training Tokens]_[Training Partition]_[Random Seed]-d[Dropout Rate].pt`\n",
    "\n",
    "The following analysis was done on models with 3 different hidden sizes [100/400/1600]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model_100_file = './data/LSTM_40m/LSTM_100_40m_a_0-d0.2.pt'\n",
    "model_400_file = './data/LSTM_40m/LSTM_400_40m_a_10-d0.2.pt'\n",
    "model_1600_file = './data/LSTM_40m/LSTM_1600_40m_a_20-d0.2.pt'\n",
    "\n",
    "model_100 = torch.load(model_100_file, map_location=torch.device('cpu'))\n",
    "model_100.eval()\n",
    "\n",
    "model_400 = torch.load(model_400_file, map_location=torch.device('cpu'))\n",
    "model_400.eval()\n",
    "\n",
    "model_1600 = torch.load(model_1600_file, map_location=torch.device('cpu'))\n",
    "model_1600.eval()\n",
    "\n",
    "\n",
    "print(model_100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RNNModel(\n",
      "  (drop): Dropout(p=0.2, inplace=False)\n",
      "  (encoder): Embedding(28439, 100)\n",
      "  (rnn): LSTM(100, 100, num_layers=2, dropout=0.2)\n",
      "  (decoder): Linear(in_features=100, out_features=28439, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load vocab and text files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load and preprocess the story file. (Lowercase and append \"EOS\")\n",
    "\n",
    "Mark highlighted sentences"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "story_file = \"./data/Full Story_So much water so close to home_Highlighted.docx\"\n",
    "vocab_file = \"./data/vocab.txt\"\n",
    "\n",
    "vocab = Dictionary(vocab_file)\n",
    "story_text, is_highlight = preprocess(story_file)\n",
    "processed_story, story_sents = word2idx(story_text, vocab)\n",
    "\n",
    "sample_sent_idx = 2\n",
    "print(story_text[sample_sent_idx])\n",
    "print(processed_story[sample_sent_idx])\n",
    "print(is_highlight[sample_sent_idx])\n",
    "print(story_sents[sample_sent_idx])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "He chews, arms on the table, and stares at something across the room.\n",
      "['he', '<unk>', ',', 'arms', 'on', 'the', 'table', ',', 'and', '<unk>', 'at', 'something', 'across', 'the', 'room', '.', '<eos>']\n",
      "False\n",
      "tensor([[   18],\n",
      "        [28437],\n",
      "        [    1],\n",
      "        [ 1124],\n",
      "        [   13],\n",
      "        [    0],\n",
      "        [ 1571],\n",
      "        [    1],\n",
      "        [    5],\n",
      "        [28437],\n",
      "        [   22],\n",
      "        [  574],\n",
      "        [  475],\n",
      "        [    0],\n",
      "        [  804],\n",
      "        [    2],\n",
      "        [28438]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To test the validity of `sent_perplexity`, run each model again on an excerpt from [test.txt](https://github.com/vansky/neural-complexity/blob/master/data/wikitext-2/test.txt) used to evaluate the language model. (From wikitext-2 dataset)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "test_file = \"./data/test.txt\"\n",
    "with open(test_file, \"r\") as text_file:\n",
    "    test_text = text_file.read().replace('\\n', '')\n",
    "test_text = sent_tokenize(test_text)\n",
    "processed_test, test_sents = word2idx(test_text, vocab)\n",
    "\n",
    "print(test_text[1])\n",
    "print(processed_test[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "It caused enormous disruption to Chinese society : the census of 754 recorded 52 @.\n",
      "['it', 'caused', 'enormous', 'disruption', 'to', '<unk>', 'society', ':', 'the', 'census', 'of', '<unk>', 'recorded', '<unk>', '@', '.', '<eos>']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate PPL per sentence\n",
    "\n",
    "Calculate sentence PPL for each highlight and averaged over them.\n",
    "\n",
    "For each sentence, PPL is calculated as `exp(cross_entropy_loss[prediction, target])`, where `prediction` includes output from reading w[-1] (the last output from the previous sentence) all the way to w[n-2] and `target` includes w[0],...,w[n-1] (the last word of the current sentence, which is always 'EOS' now after preprocessing.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from utils.analysis import sent_perplexity\n",
    "models = [model_100, model_400, model_1600]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Story text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "with torch.no_grad():\n",
    "    for model in models:\n",
    "        ppl_sent_highlight = []\n",
    "        hidden_size = model.nhid\n",
    "        for i, sent in enumerate(story_sents):\n",
    "            if i==0:\n",
    "                hidden = model.init_hidden(bsz=1)\n",
    "                out, hidden = model(sent, hidden)\n",
    "            out_prev = out[-1, 0] #TODO: optimize memory\n",
    "            ppl, out, hidden = sent_perplexity(sent, model, vocab, out_prev, hidden)\n",
    "            if is_highlight[i]:\n",
    "                ppl_sent_highlight.append(ppl)\n",
    "\n",
    "        print(f\"Model_{hidden_size} ppl {np.mean(ppl_sent_highlight)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model_100 ppl 195.86708068847656\n",
      "Model_400 ppl 114.10734558105469\n",
      "Model_1600 ppl 92.59598541259766\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test text from wikitext-2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "with torch.no_grad():\n",
    "    for model in models:\n",
    "        ppl_sent = []\n",
    "        hidden_size = model.nhid\n",
    "        for i, sent in enumerate(test_sents):\n",
    "            if i==0:\n",
    "                hidden = model.init_hidden(bsz=1)\n",
    "                out, hidden = model(sent, hidden)\n",
    "            out_prev = out[-1, 0] #TODO: optimize memory\n",
    "            ppl, out, hidden = sent_perplexity(sent, model, vocab, out_prev, hidden)\n",
    "            if is_highlight[i]:\n",
    "                ppl_sent.append(ppl)\n",
    "\n",
    "        print(f\"Model_{hidden_size} ppl {np.mean(ppl_sent)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model_100 ppl 59.82924270629883\n",
      "Model_400 ppl 36.10517883300781\n",
      "Model_1600 ppl 30.701034545898438\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('surprisal': conda)"
  },
  "interpreter": {
   "hash": "e1e7eae36acc1650c186ae862cd3425b929bae7d94f549ab18ed00cf64c22626"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}